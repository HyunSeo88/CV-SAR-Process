{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790c34f7",
   "metadata": {},
   "source": [
    "모델이 초해상화한 패치와 Ground Truth 패치를 비교한다.\n",
    "현재 graph1, patch_extractor_gpu_enhanced.py를 이용하여 위상 정보를 살린 보정을 진행하였기 때문에, 우리가 흔히 아는 SAR 이미지로 결과를 비교하기 위해서는 두 patch에 대해 나머지 보정을 해야한다.\n",
    "각 노트북 셀은 순차적으로 특정 패치에 대한 보정 작업을 실시한다. 마지막셀에서는 두 패치의 결과를 시각화한 것을 하나의 이미지 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0629da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 설정\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model type: swin\n",
      "Input patch size: (64, 128)\n",
      "Target patch size: (256, 512)\n",
      "Super-resolution factor: 4x\n"
     ]
    }
   ],
   "source": [
    "# 경로 설정\n",
    "data_path = '../data/processed_2'\n",
    "model_path = '../model'\n",
    "output_path = '../analysis_results'\n",
    "\n",
    "# 모델 설정\n",
    "model_type = 'unet'  # 'unet' 또는 'swin'\n",
    "model_weights = '../model_weights/version2/best_model.pth'  # 학습된 모델 가중치 경로\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 패치 설정\n",
    "patch_size = (64, 128)  # 입력 패치 크기 (H, W)\n",
    "sr_factor = 4           # 초해상화 배율\n",
    "target_patch_size = (patch_size[0] * sr_factor, patch_size[1] * sr_factor)\n",
    "\n",
    "# 시각화 설정\n",
    "figsize = (15, 10)\n",
    "cmap = 'gray'\n",
    "save_results = True\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model type: {model_type}\")\n",
    "print(f\"Input patch size: {patch_size}\")\n",
    "print(f\"Target patch size: {target_patch_size}\")\n",
    "print(f\"Super-resolution factor: {sr_factor}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩 및 유틸리티 함수\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(model_path)\n",
    "\n",
    "if model_type == 'swin':\n",
    "    from ac_swin_unet_pp import create_model\n",
    "else:\n",
    "    from cv_unet import ComplexUNet as create_model\n",
    "\n",
    "from utils import sr_loss, MetricsCalculator\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"학습된 모델 로드\"\"\"\n",
    "    model = create_model()\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        print(f\"✓ Model loaded from {model_path}\")\n",
    "    else:\n",
    "        print(f\"⚠ Model weights not found at {model_path}, using random weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def simulate_degradation(hr_patch, scale_factor=4):\n",
    "    \"\"\"HR 패치를 LR로 열화시키는 함수 (모델 학습과 동일한 방식)\"\"\"\n",
    "    # 가우시안 블러 + 다운샘플링\n",
    "    hr_tensor = torch.from_numpy(hr_patch).float().unsqueeze(0)\n",
    "    \n",
    "    # 안티앨리어싱 블러\n",
    "    blurred = torch.nn.functional.interpolate(\n",
    "        hr_tensor, \n",
    "        scale_factor=1/scale_factor, \n",
    "        mode='bilinear', \n",
    "        align_corners=False,\n",
    "        antialias=True\n",
    "    )\n",
    "    \n",
    "    return blurred.squeeze(0).numpy()\n",
    "\n",
    "def complex_to_intensity(complex_data):\n",
    "    \"\"\"복소수 SAR 데이터를 강도 이미지로 변환\"\"\"\n",
    "    if complex_data.dtype == np.complex64 or complex_data.dtype == np.complex128:\n",
    "        return np.abs(complex_data)\n",
    "    else:\n",
    "        # 4채널 실수 데이터 (VV_real, VV_imag, VH_real, VH_imag)\n",
    "        vv_complex = complex_data[0] + 1j * complex_data[1]\n",
    "        vh_complex = complex_data[2] + 1j * complex_data[3]\n",
    "        return np.stack([np.abs(vv_complex), np.abs(vh_complex)], axis=0)\n",
    "\n",
    "def apply_sar_corrections(intensity_data, log_scale=True, percentile_norm=True):\n",
    "    \"\"\"SAR 이미지 시각화를 위한 보정\"\"\"\n",
    "    # 로그 스케일 적용\n",
    "    if log_scale:\n",
    "        intensity_data = 10 * np.log10(intensity_data + 1e-10)\n",
    "    \n",
    "    # 백분위 정규화\n",
    "    if percentile_norm:\n",
    "        p2, p98 = np.percentile(intensity_data, [2, 98])\n",
    "        intensity_data = np.clip((intensity_data - p2) / (p98 - p2), 0, 1)\n",
    "    \n",
    "    return intensity_data\n",
    "\n",
    "# 모델 로드\n",
    "model = load_model(model_weights, device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패치 데이터 로딩 및 선택\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_available_patches(data_dir):\n",
    "    \"\"\"사용 가능한 패치 파일들을 찾기\"\"\"\n",
    "    pattern = \"*_dual_pol_complex_*.npy\"\n",
    "    patch_files = list(Path(data_dir).rglob(pattern))\n",
    "    print(f\"Found {len(patch_files)} available patches\")\n",
    "    return patch_files\n",
    "\n",
    "def select_random_patches(patch_files, n_samples=5):\n",
    "    \"\"\"랜덤하게 패치 선택\"\"\"\n",
    "    import random\n",
    "    random.seed(42)  # 재현 가능한 결과를 위해\n",
    "    selected = random.sample(patch_files, min(n_samples, len(patch_files)))\n",
    "    return selected\n",
    "\n",
    "# 패치 파일 검색\n",
    "patch_files = load_available_patches(data_path)\n",
    "\n",
    "if len(patch_files) > 0:\n",
    "    # 테스트용 패치 선택\n",
    "    test_patches = select_random_patches(patch_files, n_samples=3)\n",
    "    print(f\"Selected {len(test_patches)} patches for testing:\")\n",
    "    for i, patch_file in enumerate(test_patches):\n",
    "        print(f\"  {i+1}. {patch_file.name}\")\n",
    "else:\n",
    "    print(\"⚠ No patch files found! Creating synthetic test data...\")\n",
    "    # 합성 테스트 데이터 생성\n",
    "    test_patches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패치 처리 및 모델 추론\n",
    "def process_single_patch(patch_file, model, device):\n",
    "    \"\"\"단일 패치에 대한 전체 처리 파이프라인\"\"\"\n",
    "    # 1. HR 패치 로드\n",
    "    try:\n",
    "        hr_patch = np.load(patch_file)\n",
    "        print(f\"Loaded patch: {hr_patch.shape}, dtype: {hr_patch.dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading patch: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 패치 크기 조정 (필요한 경우)\n",
    "    if hr_patch.shape[-2:] != target_patch_size:\n",
    "        # 패치를 target_patch_size로 크롭 또는 패딩\n",
    "        h, w = hr_patch.shape[-2:]\n",
    "        th, tw = target_patch_size\n",
    "        \n",
    "        if h >= th and w >= tw:\n",
    "            # 크롭\n",
    "            start_h = (h - th) // 2\n",
    "            start_w = (w - tw) // 2\n",
    "            hr_patch = hr_patch[..., start_h:start_h+th, start_w:start_w+tw]\n",
    "        else:\n",
    "            # 패딩 (실제로는 다른 패치 선택 권장)\n",
    "            print(f\"⚠ Patch too small: {(h,w)} < {target_patch_size}\")\n",
    "            return None\n",
    "    \n",
    "    # 3. LR 패치 생성 (열화)\n",
    "    lr_patch = simulate_degradation(hr_patch, sr_factor)\n",
    "    \n",
    "    # 4. 모델 추론\n",
    "    with torch.no_grad():\n",
    "        lr_tensor = torch.from_numpy(lr_patch).float().unsqueeze(0).to(device)\n",
    "        sr_tensor = model(lr_tensor)\n",
    "        sr_patch = sr_tensor.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'hr_original': hr_patch,\n",
    "        'lr_degraded': lr_patch,\n",
    "        'sr_predicted': sr_patch,\n",
    "        'filename': patch_file.name\n",
    "    }\n",
    "\n",
    "def create_synthetic_test_data():\n",
    "    \"\"\"실제 패치가 없을 때 합성 테스트 데이터 생성\"\"\"\n",
    "    print(\"Creating synthetic test data...\")\n",
    "    \n",
    "    # 합성 HR 패치 생성 (복잡한 텍스처 패턴)\n",
    "    h, w = target_patch_size\n",
    "    x = np.linspace(0, 4*np.pi, w)\n",
    "    y = np.linspace(0, 4*np.pi, h)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # VV 채널: 복잡한 간섭 패턴\n",
    "    vv_real = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)\n",
    "    vv_imag = np.cos(X) * np.sin(Y) + 0.3 * np.cos(2*Y - X)\n",
    "    \n",
    "    # VH 채널: 다른 패턴\n",
    "    vh_real = 0.7 * (np.sin(X + Y) + 0.5 * np.sin(3*X))\n",
    "    vh_imag = 0.7 * (np.cos(X - Y) + 0.3 * np.cos(3*Y))\n",
    "    \n",
    "    # 노이즈 추가\n",
    "    noise_level = 0.1\n",
    "    vv_real += np.random.normal(0, noise_level, vv_real.shape)\n",
    "    vv_imag += np.random.normal(0, noise_level, vv_imag.shape)\n",
    "    vh_real += np.random.normal(0, noise_level, vh_real.shape)\n",
    "    vh_imag += np.random.normal(0, noise_level, vh_imag.shape)\n",
    "    \n",
    "    hr_patch = np.stack([vv_real, vv_imag, vh_real, vh_imag], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'hr_original': hr_patch,\n",
    "        'filename': 'synthetic_test_patch.npy'\n",
    "    }\n",
    "\n",
    "# 패치 처리 실행\n",
    "results = []\n",
    "\n",
    "if test_patches is not None:\n",
    "    # 실제 패치 처리\n",
    "    for i, patch_file in enumerate(test_patches):\n",
    "        print(f\"\\n--- Processing patch {i+1}/{len(test_patches)}: {patch_file.name} ---\")\n",
    "        result = process_single_patch(patch_file, model, device)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "else:\n",
    "    # 합성 데이터 처리\n",
    "    print(\"\\n--- Processing synthetic test data ---\")\n",
    "    synthetic_data = create_synthetic_test_data()\n",
    "    \n",
    "    # 열화 및 추론 적용\n",
    "    lr_patch = simulate_degradation(synthetic_data['hr_original'], sr_factor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lr_tensor = torch.from_numpy(lr_patch).float().unsqueeze(0).to(device)\n",
    "        sr_tensor = model(lr_tensor)\n",
    "        sr_patch = sr_tensor.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    result = {\n",
    "        'hr_original': synthetic_data['hr_original'],\n",
    "        'lr_degraded': lr_patch,\n",
    "        'sr_predicted': sr_patch,\n",
    "        'filename': synthetic_data['filename']\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\n✓ Successfully processed {len(results)} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fece0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR 이미지 보정 및 시각화\n",
    "def apply_full_sar_corrections(complex_data, apply_speckle_filter=True):\n",
    "    \"\"\"완전한 SAR 이미지 보정 파이프라인\"\"\"\n",
    "    \n",
    "    # 1. 복소수 → 강도 변환\n",
    "    intensity = complex_to_intensity(complex_data)\n",
    "    \n",
    "    # 2. 다중편파 강도 계산\n",
    "    if intensity.ndim == 3 and intensity.shape[0] == 2:\n",
    "        vv_intensity = intensity[0]\n",
    "        vh_intensity = intensity[1]\n",
    "        \n",
    "        # Pauli decomposition 또는 RGB 합성\n",
    "        # 여기서는 간단한 RGB 합성 사용\n",
    "        rgb_composite = np.stack([\n",
    "            vv_intensity,  # R: VV\n",
    "            vh_intensity,  # G: VH  \n",
    "            vv_intensity + vh_intensity  # B: Total power\n",
    "        ], axis=0)\n",
    "        intensity = rgb_composite\n",
    "    \n",
    "    # 3. 스페클 필터링 (옵션)\n",
    "    if apply_speckle_filter:\n",
    "        from scipy import ndimage\n",
    "        if intensity.ndim == 3:\n",
    "            for i in range(intensity.shape[0]):\n",
    "                intensity[i] = ndimage.median_filter(intensity[i], size=3)\n",
    "        else:\n",
    "            intensity = ndimage.median_filter(intensity, size=3)\n",
    "    \n",
    "    # 4. 로그 스케일 변환\n",
    "    intensity_db = 10 * np.log10(intensity + 1e-10)\n",
    "    \n",
    "    # 5. 히스토그램 이퀄라이제이션\n",
    "    from scipy.stats import rankdata\n",
    "    if intensity_db.ndim == 3:\n",
    "        for i in range(intensity_db.shape[0]):\n",
    "            flat = intensity_db[i].flatten()\n",
    "            ranks = rankdata(flat, method='dense') - 1\n",
    "            intensity_db[i] = (ranks / ranks.max()).reshape(intensity_db[i].shape)\n",
    "    else:\n",
    "        flat = intensity_db.flatten()\n",
    "        ranks = rankdata(flat, method='dense') - 1\n",
    "        intensity_db = (ranks / ranks.max()).reshape(intensity_db.shape)\n",
    "    \n",
    "    # 6. 대비 향상\n",
    "    p1, p99 = np.percentile(intensity_db, [1, 99])\n",
    "    intensity_enhanced = np.clip((intensity_db - p1) / (p99 - p1), 0, 1)\n",
    "    \n",
    "    return intensity_enhanced\n",
    "\n",
    "def calculate_metrics(hr_patch, sr_patch):\n",
    "    \"\"\"이미지 품질 메트릭 계산\"\"\"\n",
    "    \n",
    "    # 동일한 크기로 맞춤\n",
    "    if hr_patch.shape != sr_patch.shape:\n",
    "        min_h = min(hr_patch.shape[-2], sr_patch.shape[-2])\n",
    "        min_w = min(hr_patch.shape[-1], sr_patch.shape[-1])\n",
    "        hr_patch = hr_patch[..., :min_h, :min_w]\n",
    "        sr_patch = sr_patch[..., :min_h, :min_w]\n",
    "    \n",
    "    # 복소수 강도로 변환\n",
    "    hr_intensity = complex_to_intensity(hr_patch)\n",
    "    sr_intensity = complex_to_intensity(sr_patch)\n",
    "    \n",
    "    # PSNR 계산\n",
    "    mse = np.mean((hr_intensity - sr_intensity) ** 2)\n",
    "    psnr = 20 * np.log10(np.max(hr_intensity)) - 10 * np.log10(mse)\n",
    "    \n",
    "    # SSIM 계산 (간단한 버전)\n",
    "    def ssim_simple(img1, img2):\n",
    "        mu1, mu2 = np.mean(img1), np.mean(img2)\n",
    "        sigma1, sigma2 = np.std(img1), np.std(img2)\n",
    "        sigma12 = np.mean((img1 - mu1) * (img2 - mu2))\n",
    "        \n",
    "        c1, c2 = (0.01 * (np.max(img1) - np.min(img1))) ** 2, (0.03 * (np.max(img1) - np.min(img1))) ** 2\n",
    "        ssim = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2) / ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))\n",
    "        return ssim\n",
    "    \n",
    "    if hr_intensity.ndim == 3:\n",
    "        ssim_scores = [ssim_simple(hr_intensity[i], sr_intensity[i]) for i in range(hr_intensity.shape[0])]\n",
    "        ssim = np.mean(ssim_scores)\n",
    "    else:\n",
    "        ssim = ssim_simple(hr_intensity, sr_intensity)\n",
    "    \n",
    "    return {'PSNR': psnr, 'SSIM': ssim}\n",
    "\n",
    "# 각 결과에 대해 보정 적용\n",
    "print(\"Applying SAR corrections to all results...\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Processing result {i+1}/{len(results)}: {result['filename']}\")\n",
    "    \n",
    "    # 보정 적용\n",
    "    result['hr_corrected'] = apply_full_sar_corrections(result['hr_original'])\n",
    "    result['lr_corrected'] = apply_full_sar_corrections(result['lr_degraded']) \n",
    "    result['sr_corrected'] = apply_full_sar_corrections(result['sr_predicted'])\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    result['metrics'] = calculate_metrics(result['hr_original'], result['sr_predicted'])\n",
    "    \n",
    "    print(f\"  PSNR: {result['metrics']['PSNR']:.2f} dB\")\n",
    "    print(f\"  SSIM: {result['metrics']['SSIM']:.3f}\")\n",
    "\n",
    "print(\"✓ SAR corrections applied to all results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 및 비교 플롯 생성\n",
    "def create_comparison_plot(result, save_path=None):\n",
    "    \"\"\"단일 패치에 대한 비교 플롯 생성\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle(f'SAR Patch Comparison: {result[\"filename\"]}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 첫 번째 행: 원본 데이터 (복소수 강도)\n",
    "    hr_intensity = complex_to_intensity(result['hr_original'])\n",
    "    lr_intensity = complex_to_intensity(result['lr_degraded'])  \n",
    "    sr_intensity = complex_to_intensity(result['sr_predicted'])\n",
    "    \n",
    "    # VV 채널만 표시 (다중채널인 경우)\n",
    "    if hr_intensity.ndim == 3:\n",
    "        hr_show = hr_intensity[0]  # VV 채널\n",
    "        lr_show = lr_intensity[0]  \n",
    "        sr_show = sr_intensity[0]\n",
    "    else:\n",
    "        hr_show = hr_intensity\n",
    "        lr_show = lr_intensity\n",
    "        sr_show = sr_intensity\n",
    "    \n",
    "    # 첫 번째 행: 강도 이미지\n",
    "    im1 = axes[0,0].imshow(hr_show, cmap='gray', vmin=np.percentile(hr_show, 1), vmax=np.percentile(hr_show, 99))\n",
    "    axes[0,0].set_title('HR Original (Intensity)', fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0,0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im2 = axes[0,1].imshow(lr_show, cmap='gray', vmin=np.percentile(lr_show, 1), vmax=np.percentile(lr_show, 99))\n",
    "    axes[0,1].set_title('LR Degraded (Intensity)', fontweight='bold')\n",
    "    axes[0,1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im3 = axes[0,2].imshow(sr_show, cmap='gray', vmin=np.percentile(sr_show, 1), vmax=np.percentile(sr_show, 99))\n",
    "    axes[0,2].set_title('SR Predicted (Intensity)', fontweight='bold')\n",
    "    axes[0,2].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[0,2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 차이 이미지\n",
    "    if hr_show.shape == sr_show.shape:\n",
    "        diff = np.abs(hr_show - sr_show)\n",
    "        im4 = axes[0,3].imshow(diff, cmap='hot', vmin=0, vmax=np.percentile(diff, 95))\n",
    "        axes[0,3].set_title('|HR - SR| Difference', fontweight='bold')\n",
    "        axes[0,3].axis('off')\n",
    "        plt.colorbar(im4, ax=axes[0,3], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[0,3].text(0.5, 0.5, 'Size Mismatch', ha='center', va='center', transform=axes[0,3].transAxes)\n",
    "        axes[0,3].set_title('Difference (N/A)', fontweight='bold')\n",
    "        axes[0,3].axis('off')\n",
    "    \n",
    "    # 두 번째 행: 보정된 SAR 이미지\n",
    "    hr_corrected = result['hr_corrected']\n",
    "    lr_corrected = result['lr_corrected']\n",
    "    sr_corrected = result['sr_corrected']\n",
    "    \n",
    "    # 다중채널인 경우 RGB 또는 첫 번째 채널 표시\n",
    "    if hr_corrected.ndim == 3 and hr_corrected.shape[0] == 3:\n",
    "        # RGB 이미지로 표시\n",
    "        hr_vis = np.transpose(hr_corrected, (1, 2, 0))\n",
    "        lr_vis = np.transpose(lr_corrected, (1, 2, 0))\n",
    "        sr_vis = np.transpose(sr_corrected, (1, 2, 0))\n",
    "        cmap_corrected = None\n",
    "    else:\n",
    "        # 단일 채널 또는 첫 번째 채널\n",
    "        hr_vis = hr_corrected[0] if hr_corrected.ndim == 3 else hr_corrected\n",
    "        lr_vis = lr_corrected[0] if lr_corrected.ndim == 3 else lr_corrected\n",
    "        sr_vis = sr_corrected[0] if sr_corrected.ndim == 3 else sr_corrected\n",
    "        cmap_corrected = 'gray'\n",
    "    \n",
    "    axes[1,0].imshow(hr_vis, cmap=cmap_corrected)\n",
    "    axes[1,0].set_title('HR Corrected (SAR)', fontweight='bold')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    axes[1,1].imshow(lr_vis, cmap=cmap_corrected)\n",
    "    axes[1,1].set_title('LR Corrected (SAR)', fontweight='bold')  \n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    axes[1,2].imshow(sr_vis, cmap=cmap_corrected)\n",
    "    axes[1,2].set_title('SR Corrected (SAR)', fontweight='bold')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    # 메트릭 표시\n",
    "    metrics_text = f\"PSNR: {result['metrics']['PSNR']:.2f} dB\\\\nSSIM: {result['metrics']['SSIM']:.3f}\"\n",
    "    axes[1,3].text(0.1, 0.7, metrics_text, fontsize=14, fontweight='bold', \n",
    "                   transform=axes[1,3].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    \n",
    "    # 추가 정보\n",
    "    info_text = f\"Input size: {result['lr_degraded'].shape[-2:]}\\\\nOutput size: {result['sr_predicted'].shape[-2:]}\\\\nScale factor: {sr_factor}x\"\n",
    "    axes[1,3].text(0.1, 0.3, info_text, fontsize=12, transform=axes[1,3].transAxes,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    \n",
    "    axes[1,3].set_title('Metrics & Info', fontweight='bold')\n",
    "    axes[1,3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved comparison plot: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 모든 결과에 대해 시각화 생성\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"Creating visualization plots for {len(results)} results...\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\\\n--- Creating plot {i+1}/{len(results)}: {result['filename']} ---\")\n",
    "    \n",
    "    if save_results:\n",
    "        # 파일명에서 확장자 제거하고 비교 플롯 이름 생성\n",
    "        base_name = os.path.splitext(result['filename'])[0]\n",
    "        save_path = os.path.join(output_path, f'sar_comparison_{base_name}.png')\n",
    "    else:\n",
    "        save_path = None\n",
    "    \n",
    "    fig = create_comparison_plot(result, save_path)\n",
    "    \n",
    "    # Jupyter에서 표시\n",
    "    plt.show()\n",
    "    \n",
    "    # 메모리 정리\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\\\n✓ All visualizations completed!\")\n",
    "print(f\"Results saved in: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 통계 및 최종 분석\n",
    "import pandas as pd\n",
    "\n",
    "def create_summary_report(results):\n",
    "    \"\"\"결과 요약 리포트 생성\"\"\"\n",
    "    \n",
    "    # 메트릭 데이터 수집\n",
    "    summary_data = []\n",
    "    for result in results:\n",
    "        summary_data.append({\n",
    "            'Filename': result['filename'],\n",
    "            'PSNR (dB)': result['metrics']['PSNR'],\n",
    "            'SSIM': result['metrics']['SSIM'],\n",
    "            'HR_Shape': str(result['hr_original'].shape),\n",
    "            'LR_Shape': str(result['lr_degraded'].shape),\n",
    "            'SR_Shape': str(result['sr_predicted'].shape)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # 통계 요약\n",
    "    print(\"=== SAR Super-Resolution Performance Summary ===\")\n",
    "    print(f\"Number of test patches: {len(results)}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Super-resolution factor: {sr_factor}x\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📊 Performance Metrics:\")\n",
    "    print(f\"Average PSNR: {df['PSNR (dB)'].mean():.2f} ± {df['PSNR (dB)'].std():.2f} dB\")\n",
    "    print(f\"Average SSIM: {df['SSIM'].mean():.3f} ± {df['SSIM'].std():.3f}\")\n",
    "    print(f\"Best PSNR: {df['PSNR (dB)'].max():.2f} dB ({df.loc[df['PSNR (dB)'].idxmax(), 'Filename']})\")\n",
    "    print(f\"Best SSIM: {df['SSIM'].max():.3f} ({df.loc[df['SSIM'].idxmax(), 'Filename']})\")\n",
    "    print()\n",
    "    \n",
    "    # 상세 결과 테이블\n",
    "    print(\"📋 Detailed Results:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_summary_visualization(results):\n",
    "    \"\"\"요약 시각화 생성\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('SAR Super-Resolution Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # PSNR 분포\n",
    "    psnr_values = [r['metrics']['PSNR'] for r in results]\n",
    "    axes[0,0].hist(psnr_values, bins=max(3, len(results)//2), alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('PSNR Distribution')\n",
    "    axes[0,0].set_xlabel('PSNR (dB)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SSIM 분포  \n",
    "    ssim_values = [r['metrics']['SSIM'] for r in results]\n",
    "    axes[0,1].hist(ssim_values, bins=max(3, len(results)//2), alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[0,1].set_title('SSIM Distribution') \n",
    "    axes[0,1].set_xlabel('SSIM')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PSNR vs SSIM 산점도\n",
    "    axes[1,0].scatter(psnr_values, ssim_values, alpha=0.7, s=100, c='red')\n",
    "    axes[1,0].set_title('PSNR vs SSIM Correlation')\n",
    "    axes[1,0].set_xlabel('PSNR (dB)')\n",
    "    axes[1,0].set_ylabel('SSIM')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 패치별 성능 비교\n",
    "    patch_names = [r['filename'][:20] + '...' if len(r['filename']) > 20 else r['filename'] for r in results]\n",
    "    x_pos = np.arange(len(patch_names))\n",
    "    \n",
    "    ax1 = axes[1,1]\n",
    "    bars1 = ax1.bar(x_pos - 0.2, psnr_values, 0.4, label='PSNR', alpha=0.7)\n",
    "    ax1.set_ylabel('PSNR (dB)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x_pos + 0.2, ssim_values, 0.4, label='SSIM', alpha=0.7, color='orange')\n",
    "    ax2.set_ylabel('SSIM', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax1.set_title('Per-Patch Performance')\n",
    "    ax1.set_xlabel('Patch')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(patch_names, rotation=45, ha='right')\n",
    "    \n",
    "    # 범례\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_results:\n",
    "        summary_plot_path = os.path.join(output_path, 'sar_performance_summary.png')\n",
    "        plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved summary plot: {summary_plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 요약 분석 실행\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 요약 리포트 생성\n",
    "df_summary = create_summary_report(results)\n",
    "\n",
    "# 요약 시각화 생성\n",
    "summary_fig = create_summary_visualization(results)\n",
    "\n",
    "# CSV로 결과 저장\n",
    "if save_results:\n",
    "    csv_path = os.path.join(output_path, 'sar_performance_metrics.csv')\n",
    "    df_summary.to_csv(csv_path, index=False)\n",
    "    print(f\"\\\\n✓ Saved metrics to: {csv_path}\")\n",
    "\n",
    "print(f\"\\\\n🎉 Analysis complete! Check {output_path} for all generated files.\")\n",
    "\n",
    "# 메모리 정리\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
