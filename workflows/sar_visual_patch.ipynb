{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790c34f7",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ì´ˆí•´ìƒí™”í•œ íŒ¨ì¹˜ì™€ Ground Truth íŒ¨ì¹˜ë¥¼ ë¹„êµí•œë‹¤.\n",
    "í˜„ì¬ graph1, patch_extractor_gpu_enhanced.pyë¥¼ ì´ìš©í•˜ì—¬ ìœ„ìƒ ì •ë³´ë¥¼ ì‚´ë¦° ë³´ì •ì„ ì§„í–‰í•˜ì˜€ê¸° ë•Œë¬¸ì—, ìš°ë¦¬ê°€ í”íˆ ì•„ëŠ” SAR ì´ë¯¸ì§€ë¡œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ì„œëŠ” ë‘ patchì— ëŒ€í•´ ë‚˜ë¨¸ì§€ ë³´ì •ì„ í•´ì•¼í•œë‹¤.\n",
    "ê° ë…¸íŠ¸ë¶ ì…€ì€ ìˆœì°¨ì ìœ¼ë¡œ íŠ¹ì • íŒ¨ì¹˜ì— ëŒ€í•œ ë³´ì • ì‘ì—…ì„ ì‹¤ì‹œí•œë‹¤. ë§ˆì§€ë§‰ì…€ì—ì„œëŠ” ë‘ íŒ¨ì¹˜ì˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ ê²ƒì„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0629da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model type: swin\n",
      "Input patch size: (64, 128)\n",
      "Target patch size: (256, 512)\n",
      "Super-resolution factor: 4x\n"
     ]
    }
   ],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "data_path = '../data/processed_2'\n",
    "model_path = '../model'\n",
    "output_path = '../analysis_results'\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model_type = 'unet'  # 'unet' ë˜ëŠ” 'swin'\n",
    "model_weights = '../model_weights/version2/best_model.pth'  # í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# íŒ¨ì¹˜ ì„¤ì •\n",
    "patch_size = (64, 128)  # ì…ë ¥ íŒ¨ì¹˜ í¬ê¸° (H, W)\n",
    "sr_factor = 4           # ì´ˆí•´ìƒí™” ë°°ìœ¨\n",
    "target_patch_size = (patch_size[0] * sr_factor, patch_size[1] * sr_factor)\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "figsize = (15, 10)\n",
    "cmap = 'gray'\n",
    "save_results = True\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model type: {model_type}\")\n",
    "print(f\"Input patch size: {patch_size}\")\n",
    "print(f\"Target patch size: {target_patch_size}\")\n",
    "print(f\"Super-resolution factor: {sr_factor}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë”© ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(model_path)\n",
    "\n",
    "if model_type == 'swin':\n",
    "    from ac_swin_unet_pp import create_model\n",
    "else:\n",
    "    from cv_unet import ComplexUNet as create_model\n",
    "\n",
    "from utils import sr_loss, MetricsCalculator\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    model = create_model()\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        print(f\"âœ“ Model loaded from {model_path}\")\n",
    "    else:\n",
    "        print(f\"âš  Model weights not found at {model_path}, using random weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def simulate_degradation(hr_patch, scale_factor=4):\n",
    "    \"\"\"HR íŒ¨ì¹˜ë¥¼ LRë¡œ ì—´í™”ì‹œí‚¤ëŠ” í•¨ìˆ˜ (ëª¨ë¸ í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹)\"\"\"\n",
    "    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ + ë‹¤ìš´ìƒ˜í”Œë§\n",
    "    hr_tensor = torch.from_numpy(hr_patch).float().unsqueeze(0)\n",
    "    \n",
    "    # ì•ˆí‹°ì•¨ë¦¬ì–´ì‹± ë¸”ëŸ¬\n",
    "    blurred = torch.nn.functional.interpolate(\n",
    "        hr_tensor, \n",
    "        scale_factor=1/scale_factor, \n",
    "        mode='bilinear', \n",
    "        align_corners=False,\n",
    "        antialias=True\n",
    "    )\n",
    "    \n",
    "    return blurred.squeeze(0).numpy()\n",
    "\n",
    "def complex_to_intensity(complex_data):\n",
    "    \"\"\"ë³µì†Œìˆ˜ SAR ë°ì´í„°ë¥¼ ê°•ë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜\"\"\"\n",
    "    if complex_data.dtype == np.complex64 or complex_data.dtype == np.complex128:\n",
    "        return np.abs(complex_data)\n",
    "    else:\n",
    "        # 4ì±„ë„ ì‹¤ìˆ˜ ë°ì´í„° (VV_real, VV_imag, VH_real, VH_imag)\n",
    "        vv_complex = complex_data[0] + 1j * complex_data[1]\n",
    "        vh_complex = complex_data[2] + 1j * complex_data[3]\n",
    "        return np.stack([np.abs(vv_complex), np.abs(vh_complex)], axis=0)\n",
    "\n",
    "def apply_sar_corrections(intensity_data, log_scale=True, percentile_norm=True):\n",
    "    \"\"\"SAR ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•œ ë³´ì •\"\"\"\n",
    "    # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©\n",
    "    if log_scale:\n",
    "        intensity_data = 10 * np.log10(intensity_data + 1e-10)\n",
    "    \n",
    "    # ë°±ë¶„ìœ„ ì •ê·œí™”\n",
    "    if percentile_norm:\n",
    "        p2, p98 = np.percentile(intensity_data, [2, 98])\n",
    "        intensity_data = np.clip((intensity_data - p2) / (p98 - p2), 0, 1)\n",
    "    \n",
    "    return intensity_data\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = load_model(model_weights, device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨ì¹˜ ë°ì´í„° ë¡œë”© ë° ì„ íƒ\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_available_patches(data_dir):\n",
    "    \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨ì¹˜ íŒŒì¼ë“¤ì„ ì°¾ê¸°\"\"\"\n",
    "    pattern = \"*_dual_pol_complex_*.npy\"\n",
    "    patch_files = list(Path(data_dir).rglob(pattern))\n",
    "    print(f\"Found {len(patch_files)} available patches\")\n",
    "    return patch_files\n",
    "\n",
    "def select_random_patches(patch_files, n_samples=5):\n",
    "    \"\"\"ëœë¤í•˜ê²Œ íŒ¨ì¹˜ ì„ íƒ\"\"\"\n",
    "    import random\n",
    "    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´\n",
    "    selected = random.sample(patch_files, min(n_samples, len(patch_files)))\n",
    "    return selected\n",
    "\n",
    "# íŒ¨ì¹˜ íŒŒì¼ ê²€ìƒ‰\n",
    "patch_files = load_available_patches(data_path)\n",
    "\n",
    "if len(patch_files) > 0:\n",
    "    # í…ŒìŠ¤íŠ¸ìš© íŒ¨ì¹˜ ì„ íƒ\n",
    "    test_patches = select_random_patches(patch_files, n_samples=3)\n",
    "    print(f\"Selected {len(test_patches)} patches for testing:\")\n",
    "    for i, patch_file in enumerate(test_patches):\n",
    "        print(f\"  {i+1}. {patch_file.name}\")\n",
    "else:\n",
    "    print(\"âš  No patch files found! Creating synthetic test data...\")\n",
    "    # í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "    test_patches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨ì¹˜ ì²˜ë¦¬ ë° ëª¨ë¸ ì¶”ë¡ \n",
    "def process_single_patch(patch_file, model, device):\n",
    "    \"\"\"ë‹¨ì¼ íŒ¨ì¹˜ì— ëŒ€í•œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    # 1. HR íŒ¨ì¹˜ ë¡œë“œ\n",
    "    try:\n",
    "        hr_patch = np.load(patch_file)\n",
    "        print(f\"Loaded patch: {hr_patch.shape}, dtype: {hr_patch.dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading patch: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. íŒ¨ì¹˜ í¬ê¸° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)\n",
    "    if hr_patch.shape[-2:] != target_patch_size:\n",
    "        # íŒ¨ì¹˜ë¥¼ target_patch_sizeë¡œ í¬ë¡­ ë˜ëŠ” íŒ¨ë”©\n",
    "        h, w = hr_patch.shape[-2:]\n",
    "        th, tw = target_patch_size\n",
    "        \n",
    "        if h >= th and w >= tw:\n",
    "            # í¬ë¡­\n",
    "            start_h = (h - th) // 2\n",
    "            start_w = (w - tw) // 2\n",
    "            hr_patch = hr_patch[..., start_h:start_h+th, start_w:start_w+tw]\n",
    "        else:\n",
    "            # íŒ¨ë”© (ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ íŒ¨ì¹˜ ì„ íƒ ê¶Œì¥)\n",
    "            print(f\"âš  Patch too small: {(h,w)} < {target_patch_size}\")\n",
    "            return None\n",
    "    \n",
    "    # 3. LR íŒ¨ì¹˜ ìƒì„± (ì—´í™”)\n",
    "    lr_patch = simulate_degradation(hr_patch, sr_factor)\n",
    "    \n",
    "    # 4. ëª¨ë¸ ì¶”ë¡ \n",
    "    with torch.no_grad():\n",
    "        lr_tensor = torch.from_numpy(lr_patch).float().unsqueeze(0).to(device)\n",
    "        sr_tensor = model(lr_tensor)\n",
    "        sr_patch = sr_tensor.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'hr_original': hr_patch,\n",
    "        'lr_degraded': lr_patch,\n",
    "        'sr_predicted': sr_patch,\n",
    "        'filename': patch_file.name\n",
    "    }\n",
    "\n",
    "def create_synthetic_test_data():\n",
    "    \"\"\"ì‹¤ì œ íŒ¨ì¹˜ê°€ ì—†ì„ ë•Œ í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    print(\"Creating synthetic test data...\")\n",
    "    \n",
    "    # í•©ì„± HR íŒ¨ì¹˜ ìƒì„± (ë³µì¡í•œ í…ìŠ¤ì²˜ íŒ¨í„´)\n",
    "    h, w = target_patch_size\n",
    "    x = np.linspace(0, 4*np.pi, w)\n",
    "    y = np.linspace(0, 4*np.pi, h)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # VV ì±„ë„: ë³µì¡í•œ ê°„ì„­ íŒ¨í„´\n",
    "    vv_real = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)\n",
    "    vv_imag = np.cos(X) * np.sin(Y) + 0.3 * np.cos(2*Y - X)\n",
    "    \n",
    "    # VH ì±„ë„: ë‹¤ë¥¸ íŒ¨í„´\n",
    "    vh_real = 0.7 * (np.sin(X + Y) + 0.5 * np.sin(3*X))\n",
    "    vh_imag = 0.7 * (np.cos(X - Y) + 0.3 * np.cos(3*Y))\n",
    "    \n",
    "    # ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "    noise_level = 0.1\n",
    "    vv_real += np.random.normal(0, noise_level, vv_real.shape)\n",
    "    vv_imag += np.random.normal(0, noise_level, vv_imag.shape)\n",
    "    vh_real += np.random.normal(0, noise_level, vh_real.shape)\n",
    "    vh_imag += np.random.normal(0, noise_level, vh_imag.shape)\n",
    "    \n",
    "    hr_patch = np.stack([vv_real, vv_imag, vh_real, vh_imag], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'hr_original': hr_patch,\n",
    "        'filename': 'synthetic_test_patch.npy'\n",
    "    }\n",
    "\n",
    "# íŒ¨ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "results = []\n",
    "\n",
    "if test_patches is not None:\n",
    "    # ì‹¤ì œ íŒ¨ì¹˜ ì²˜ë¦¬\n",
    "    for i, patch_file in enumerate(test_patches):\n",
    "        print(f\"\\n--- Processing patch {i+1}/{len(test_patches)}: {patch_file.name} ---\")\n",
    "        result = process_single_patch(patch_file, model, device)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "else:\n",
    "    # í•©ì„± ë°ì´í„° ì²˜ë¦¬\n",
    "    print(\"\\n--- Processing synthetic test data ---\")\n",
    "    synthetic_data = create_synthetic_test_data()\n",
    "    \n",
    "    # ì—´í™” ë° ì¶”ë¡  ì ìš©\n",
    "    lr_patch = simulate_degradation(synthetic_data['hr_original'], sr_factor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lr_tensor = torch.from_numpy(lr_patch).float().unsqueeze(0).to(device)\n",
    "        sr_tensor = model(lr_tensor)\n",
    "        sr_patch = sr_tensor.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    result = {\n",
    "        'hr_original': synthetic_data['hr_original'],\n",
    "        'lr_degraded': lr_patch,\n",
    "        'sr_predicted': sr_patch,\n",
    "        'filename': synthetic_data['filename']\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\nâœ“ Successfully processed {len(results)} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fece0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR ì´ë¯¸ì§€ ë³´ì • ë° ì‹œê°í™”\n",
    "def apply_full_sar_corrections(complex_data, apply_speckle_filter=True):\n",
    "    \"\"\"ì™„ì „í•œ SAR ì´ë¯¸ì§€ ë³´ì • íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    # 1. ë³µì†Œìˆ˜ â†’ ê°•ë„ ë³€í™˜\n",
    "    intensity = complex_to_intensity(complex_data)\n",
    "    \n",
    "    # 2. ë‹¤ì¤‘í¸íŒŒ ê°•ë„ ê³„ì‚°\n",
    "    if intensity.ndim == 3 and intensity.shape[0] == 2:\n",
    "        vv_intensity = intensity[0]\n",
    "        vh_intensity = intensity[1]\n",
    "        \n",
    "        # Pauli decomposition ë˜ëŠ” RGB í•©ì„±\n",
    "        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ RGB í•©ì„± ì‚¬ìš©\n",
    "        rgb_composite = np.stack([\n",
    "            vv_intensity,  # R: VV\n",
    "            vh_intensity,  # G: VH  \n",
    "            vv_intensity + vh_intensity  # B: Total power\n",
    "        ], axis=0)\n",
    "        intensity = rgb_composite\n",
    "    \n",
    "    # 3. ìŠ¤í˜í´ í•„í„°ë§ (ì˜µì…˜)\n",
    "    if apply_speckle_filter:\n",
    "        from scipy import ndimage\n",
    "        if intensity.ndim == 3:\n",
    "            for i in range(intensity.shape[0]):\n",
    "                intensity[i] = ndimage.median_filter(intensity[i], size=3)\n",
    "        else:\n",
    "            intensity = ndimage.median_filter(intensity, size=3)\n",
    "    \n",
    "    # 4. ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³€í™˜\n",
    "    intensity_db = 10 * np.log10(intensity + 1e-10)\n",
    "    \n",
    "    # 5. íˆìŠ¤í† ê·¸ë¨ ì´í€„ë¼ì´ì œì´ì…˜\n",
    "    from scipy.stats import rankdata\n",
    "    if intensity_db.ndim == 3:\n",
    "        for i in range(intensity_db.shape[0]):\n",
    "            flat = intensity_db[i].flatten()\n",
    "            ranks = rankdata(flat, method='dense') - 1\n",
    "            intensity_db[i] = (ranks / ranks.max()).reshape(intensity_db[i].shape)\n",
    "    else:\n",
    "        flat = intensity_db.flatten()\n",
    "        ranks = rankdata(flat, method='dense') - 1\n",
    "        intensity_db = (ranks / ranks.max()).reshape(intensity_db.shape)\n",
    "    \n",
    "    # 6. ëŒ€ë¹„ í–¥ìƒ\n",
    "    p1, p99 = np.percentile(intensity_db, [1, 99])\n",
    "    intensity_enhanced = np.clip((intensity_db - p1) / (p99 - p1), 0, 1)\n",
    "    \n",
    "    return intensity_enhanced\n",
    "\n",
    "def calculate_metrics(hr_patch, sr_patch):\n",
    "    \"\"\"ì´ë¯¸ì§€ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    \n",
    "    # ë™ì¼í•œ í¬ê¸°ë¡œ ë§ì¶¤\n",
    "    if hr_patch.shape != sr_patch.shape:\n",
    "        min_h = min(hr_patch.shape[-2], sr_patch.shape[-2])\n",
    "        min_w = min(hr_patch.shape[-1], sr_patch.shape[-1])\n",
    "        hr_patch = hr_patch[..., :min_h, :min_w]\n",
    "        sr_patch = sr_patch[..., :min_h, :min_w]\n",
    "    \n",
    "    # ë³µì†Œìˆ˜ ê°•ë„ë¡œ ë³€í™˜\n",
    "    hr_intensity = complex_to_intensity(hr_patch)\n",
    "    sr_intensity = complex_to_intensity(sr_patch)\n",
    "    \n",
    "    # PSNR ê³„ì‚°\n",
    "    mse = np.mean((hr_intensity - sr_intensity) ** 2)\n",
    "    psnr = 20 * np.log10(np.max(hr_intensity)) - 10 * np.log10(mse)\n",
    "    \n",
    "    # SSIM ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)\n",
    "    def ssim_simple(img1, img2):\n",
    "        mu1, mu2 = np.mean(img1), np.mean(img2)\n",
    "        sigma1, sigma2 = np.std(img1), np.std(img2)\n",
    "        sigma12 = np.mean((img1 - mu1) * (img2 - mu2))\n",
    "        \n",
    "        c1, c2 = (0.01 * (np.max(img1) - np.min(img1))) ** 2, (0.03 * (np.max(img1) - np.min(img1))) ** 2\n",
    "        ssim = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2) / ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))\n",
    "        return ssim\n",
    "    \n",
    "    if hr_intensity.ndim == 3:\n",
    "        ssim_scores = [ssim_simple(hr_intensity[i], sr_intensity[i]) for i in range(hr_intensity.shape[0])]\n",
    "        ssim = np.mean(ssim_scores)\n",
    "    else:\n",
    "        ssim = ssim_simple(hr_intensity, sr_intensity)\n",
    "    \n",
    "    return {'PSNR': psnr, 'SSIM': ssim}\n",
    "\n",
    "# ê° ê²°ê³¼ì— ëŒ€í•´ ë³´ì • ì ìš©\n",
    "print(\"Applying SAR corrections to all results...\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Processing result {i+1}/{len(results)}: {result['filename']}\")\n",
    "    \n",
    "    # ë³´ì • ì ìš©\n",
    "    result['hr_corrected'] = apply_full_sar_corrections(result['hr_original'])\n",
    "    result['lr_corrected'] = apply_full_sar_corrections(result['lr_degraded']) \n",
    "    result['sr_corrected'] = apply_full_sar_corrections(result['sr_predicted'])\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    result['metrics'] = calculate_metrics(result['hr_original'], result['sr_predicted'])\n",
    "    \n",
    "    print(f\"  PSNR: {result['metrics']['PSNR']:.2f} dB\")\n",
    "    print(f\"  SSIM: {result['metrics']['SSIM']:.3f}\")\n",
    "\n",
    "print(\"âœ“ SAR corrections applied to all results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ë° ë¹„êµ í”Œë¡¯ ìƒì„±\n",
    "def create_comparison_plot(result, save_path=None):\n",
    "    \"\"\"ë‹¨ì¼ íŒ¨ì¹˜ì— ëŒ€í•œ ë¹„êµ í”Œë¡¯ ìƒì„±\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle(f'SAR Patch Comparison: {result[\"filename\"]}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í–‰: ì›ë³¸ ë°ì´í„° (ë³µì†Œìˆ˜ ê°•ë„)\n",
    "    hr_intensity = complex_to_intensity(result['hr_original'])\n",
    "    lr_intensity = complex_to_intensity(result['lr_degraded'])  \n",
    "    sr_intensity = complex_to_intensity(result['sr_predicted'])\n",
    "    \n",
    "    # VV ì±„ë„ë§Œ í‘œì‹œ (ë‹¤ì¤‘ì±„ë„ì¸ ê²½ìš°)\n",
    "    if hr_intensity.ndim == 3:\n",
    "        hr_show = hr_intensity[0]  # VV ì±„ë„\n",
    "        lr_show = lr_intensity[0]  \n",
    "        sr_show = sr_intensity[0]\n",
    "    else:\n",
    "        hr_show = hr_intensity\n",
    "        lr_show = lr_intensity\n",
    "        sr_show = sr_intensity\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í–‰: ê°•ë„ ì´ë¯¸ì§€\n",
    "    im1 = axes[0,0].imshow(hr_show, cmap='gray', vmin=np.percentile(hr_show, 1), vmax=np.percentile(hr_show, 99))\n",
    "    axes[0,0].set_title('HR Original (Intensity)', fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0,0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im2 = axes[0,1].imshow(lr_show, cmap='gray', vmin=np.percentile(lr_show, 1), vmax=np.percentile(lr_show, 99))\n",
    "    axes[0,1].set_title('LR Degraded (Intensity)', fontweight='bold')\n",
    "    axes[0,1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im3 = axes[0,2].imshow(sr_show, cmap='gray', vmin=np.percentile(sr_show, 1), vmax=np.percentile(sr_show, 99))\n",
    "    axes[0,2].set_title('SR Predicted (Intensity)', fontweight='bold')\n",
    "    axes[0,2].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[0,2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # ì°¨ì´ ì´ë¯¸ì§€\n",
    "    if hr_show.shape == sr_show.shape:\n",
    "        diff = np.abs(hr_show - sr_show)\n",
    "        im4 = axes[0,3].imshow(diff, cmap='hot', vmin=0, vmax=np.percentile(diff, 95))\n",
    "        axes[0,3].set_title('|HR - SR| Difference', fontweight='bold')\n",
    "        axes[0,3].axis('off')\n",
    "        plt.colorbar(im4, ax=axes[0,3], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[0,3].text(0.5, 0.5, 'Size Mismatch', ha='center', va='center', transform=axes[0,3].transAxes)\n",
    "        axes[0,3].set_title('Difference (N/A)', fontweight='bold')\n",
    "        axes[0,3].axis('off')\n",
    "    \n",
    "    # ë‘ ë²ˆì§¸ í–‰: ë³´ì •ëœ SAR ì´ë¯¸ì§€\n",
    "    hr_corrected = result['hr_corrected']\n",
    "    lr_corrected = result['lr_corrected']\n",
    "    sr_corrected = result['sr_corrected']\n",
    "    \n",
    "    # ë‹¤ì¤‘ì±„ë„ì¸ ê²½ìš° RGB ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„ í‘œì‹œ\n",
    "    if hr_corrected.ndim == 3 and hr_corrected.shape[0] == 3:\n",
    "        # RGB ì´ë¯¸ì§€ë¡œ í‘œì‹œ\n",
    "        hr_vis = np.transpose(hr_corrected, (1, 2, 0))\n",
    "        lr_vis = np.transpose(lr_corrected, (1, 2, 0))\n",
    "        sr_vis = np.transpose(sr_corrected, (1, 2, 0))\n",
    "        cmap_corrected = None\n",
    "    else:\n",
    "        # ë‹¨ì¼ ì±„ë„ ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„\n",
    "        hr_vis = hr_corrected[0] if hr_corrected.ndim == 3 else hr_corrected\n",
    "        lr_vis = lr_corrected[0] if lr_corrected.ndim == 3 else lr_corrected\n",
    "        sr_vis = sr_corrected[0] if sr_corrected.ndim == 3 else sr_corrected\n",
    "        cmap_corrected = 'gray'\n",
    "    \n",
    "    axes[1,0].imshow(hr_vis, cmap=cmap_corrected)\n",
    "    axes[1,0].set_title('HR Corrected (SAR)', fontweight='bold')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    axes[1,1].imshow(lr_vis, cmap=cmap_corrected)\n",
    "    axes[1,1].set_title('LR Corrected (SAR)', fontweight='bold')  \n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    axes[1,2].imshow(sr_vis, cmap=cmap_corrected)\n",
    "    axes[1,2].set_title('SR Corrected (SAR)', fontweight='bold')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ í‘œì‹œ\n",
    "    metrics_text = f\"PSNR: {result['metrics']['PSNR']:.2f} dB\\\\nSSIM: {result['metrics']['SSIM']:.3f}\"\n",
    "    axes[1,3].text(0.1, 0.7, metrics_text, fontsize=14, fontweight='bold', \n",
    "                   transform=axes[1,3].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    \n",
    "    # ì¶”ê°€ ì •ë³´\n",
    "    info_text = f\"Input size: {result['lr_degraded'].shape[-2:]}\\\\nOutput size: {result['sr_predicted'].shape[-2:]}\\\\nScale factor: {sr_factor}x\"\n",
    "    axes[1,3].text(0.1, 0.3, info_text, fontsize=12, transform=axes[1,3].transAxes,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    \n",
    "    axes[1,3].set_title('Metrics & Info', fontweight='bold')\n",
    "    axes[1,3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved comparison plot: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ëª¨ë“  ê²°ê³¼ì— ëŒ€í•´ ì‹œê°í™” ìƒì„±\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"Creating visualization plots for {len(results)} results...\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\\\n--- Creating plot {i+1}/{len(results)}: {result['filename']} ---\")\n",
    "    \n",
    "    if save_results:\n",
    "        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ê³  ë¹„êµ í”Œë¡¯ ì´ë¦„ ìƒì„±\n",
    "        base_name = os.path.splitext(result['filename'])[0]\n",
    "        save_path = os.path.join(output_path, f'sar_comparison_{base_name}.png')\n",
    "    else:\n",
    "        save_path = None\n",
    "    \n",
    "    fig = create_comparison_plot(result, save_path)\n",
    "    \n",
    "    # Jupyterì—ì„œ í‘œì‹œ\n",
    "    plt.show()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\\\nâœ“ All visualizations completed!\")\n",
    "print(f\"Results saved in: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš”ì•½ í†µê³„ ë° ìµœì¢… ë¶„ì„\n",
    "import pandas as pd\n",
    "\n",
    "def create_summary_report(results):\n",
    "    \"\"\"ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì§‘\n",
    "    summary_data = []\n",
    "    for result in results:\n",
    "        summary_data.append({\n",
    "            'Filename': result['filename'],\n",
    "            'PSNR (dB)': result['metrics']['PSNR'],\n",
    "            'SSIM': result['metrics']['SSIM'],\n",
    "            'HR_Shape': str(result['hr_original'].shape),\n",
    "            'LR_Shape': str(result['lr_degraded'].shape),\n",
    "            'SR_Shape': str(result['sr_predicted'].shape)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # í†µê³„ ìš”ì•½\n",
    "    print(\"=== SAR Super-Resolution Performance Summary ===\")\n",
    "    print(f\"Number of test patches: {len(results)}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Super-resolution factor: {sr_factor}x\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“Š Performance Metrics:\")\n",
    "    print(f\"Average PSNR: {df['PSNR (dB)'].mean():.2f} Â± {df['PSNR (dB)'].std():.2f} dB\")\n",
    "    print(f\"Average SSIM: {df['SSIM'].mean():.3f} Â± {df['SSIM'].std():.3f}\")\n",
    "    print(f\"Best PSNR: {df['PSNR (dB)'].max():.2f} dB ({df.loc[df['PSNR (dB)'].idxmax(), 'Filename']})\")\n",
    "    print(f\"Best SSIM: {df['SSIM'].max():.3f} ({df.loc[df['SSIM'].idxmax(), 'Filename']})\")\n",
    "    print()\n",
    "    \n",
    "    # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”\n",
    "    print(\"ğŸ“‹ Detailed Results:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_summary_visualization(results):\n",
    "    \"\"\"ìš”ì•½ ì‹œê°í™” ìƒì„±\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('SAR Super-Resolution Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # PSNR ë¶„í¬\n",
    "    psnr_values = [r['metrics']['PSNR'] for r in results]\n",
    "    axes[0,0].hist(psnr_values, bins=max(3, len(results)//2), alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('PSNR Distribution')\n",
    "    axes[0,0].set_xlabel('PSNR (dB)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SSIM ë¶„í¬  \n",
    "    ssim_values = [r['metrics']['SSIM'] for r in results]\n",
    "    axes[0,1].hist(ssim_values, bins=max(3, len(results)//2), alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[0,1].set_title('SSIM Distribution') \n",
    "    axes[0,1].set_xlabel('SSIM')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PSNR vs SSIM ì‚°ì ë„\n",
    "    axes[1,0].scatter(psnr_values, ssim_values, alpha=0.7, s=100, c='red')\n",
    "    axes[1,0].set_title('PSNR vs SSIM Correlation')\n",
    "    axes[1,0].set_xlabel('PSNR (dB)')\n",
    "    axes[1,0].set_ylabel('SSIM')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # íŒ¨ì¹˜ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "    patch_names = [r['filename'][:20] + '...' if len(r['filename']) > 20 else r['filename'] for r in results]\n",
    "    x_pos = np.arange(len(patch_names))\n",
    "    \n",
    "    ax1 = axes[1,1]\n",
    "    bars1 = ax1.bar(x_pos - 0.2, psnr_values, 0.4, label='PSNR', alpha=0.7)\n",
    "    ax1.set_ylabel('PSNR (dB)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x_pos + 0.2, ssim_values, 0.4, label='SSIM', alpha=0.7, color='orange')\n",
    "    ax2.set_ylabel('SSIM', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax1.set_title('Per-Patch Performance')\n",
    "    ax1.set_xlabel('Patch')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(patch_names, rotation=45, ha='right')\n",
    "    \n",
    "    # ë²”ë¡€\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_results:\n",
    "        summary_plot_path = os.path.join(output_path, 'sar_performance_summary.png')\n",
    "        plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved summary plot: {summary_plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ìš”ì•½ ë¶„ì„ ì‹¤í–‰\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "df_summary = create_summary_report(results)\n",
    "\n",
    "# ìš”ì•½ ì‹œê°í™” ìƒì„±\n",
    "summary_fig = create_summary_visualization(results)\n",
    "\n",
    "# CSVë¡œ ê²°ê³¼ ì €ì¥\n",
    "if save_results:\n",
    "    csv_path = os.path.join(output_path, 'sar_performance_metrics.csv')\n",
    "    df_summary.to_csv(csv_path, index=False)\n",
    "    print(f\"\\\\nâœ“ Saved metrics to: {csv_path}\")\n",
    "\n",
    "print(f\"\\\\nğŸ‰ Analysis complete! Check {output_path} for all generated files.\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
